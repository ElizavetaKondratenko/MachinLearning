{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "\"Homework2.\"",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn5BgFS7L1Uh"
      },
      "source": [
        "# Домашнее задание №2: линейная регрессия (10 баллов).\n",
        "\n",
        "Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9IIzRocL1Un"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fMTdCjXL1VD"
      },
      "source": [
        "## Многомерная линейная регрессия из sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SacKQ9nKL1VI"
      },
      "source": [
        "Применим многомерную регрессию из sklearn для стандартного датасета"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJYCMquwL1VK",
        "outputId": "916b2989-cc65-46e7-e363-42486bb487c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.datasets import make_regression\n",
        "\n",
        "X, y = make_regression(n_samples = 10000)\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 100) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl17zStoL1Vg"
      },
      "source": [
        "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWZ2_WjAL1Vq",
        "outputId": "0541481e-7855-4720-bf73-c5a1d59976f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "reg = LinearRegression().fit(X, y)\n",
        "print(mean_squared_error(y, reg.predict(X)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6011205271316267e-25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VVBoRVHL1V0"
      },
      "source": [
        "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGPRMeAjL1V-",
        "outputId": "f2661ed9-db15-484c-a8d6-df0d60f95e78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "reg = SGDRegressor(alpha=0.000000000000001).fit(X, y)\n",
        "print(mean_squared_error(y, reg.predict(X)))\n",
        "reg.coef_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.569899678588495e-25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5.51026822e+01,  1.45688919e-14, -1.32071666e-14,  7.27430776e+01,\n",
              "       -1.06257908e-14,  2.16683975e-14, -1.03835478e-14, -6.04414331e-15,\n",
              "        2.00527253e-14, -3.23683657e-14, -7.30475758e-15, -1.51118971e-14,\n",
              "        6.92692470e-15, -2.91935398e-14, -1.93710298e-14,  1.79307009e-15,\n",
              "       -1.87097836e-14,  2.90516419e-14, -4.06762976e-14,  1.80415116e-14,\n",
              "        2.81831874e-14, -2.59375325e-14, -1.88306636e-14, -7.40782111e-15,\n",
              "       -3.43590957e-14,  5.50295679e+01, -1.49607714e-14,  8.68967769e-15,\n",
              "        4.24929118e-15,  2.36999134e-14,  3.39524934e+01,  4.38418380e-14,\n",
              "        7.50789861e+01, -3.72922332e-14, -2.76751964e-15,  1.83420642e-14,\n",
              "        3.42133794e+01,  1.72162774e-14,  8.60592252e-15, -3.89588579e-14,\n",
              "        3.47937943e-16, -1.04668191e-14, -1.49578296e-14, -2.19676757e-15,\n",
              "        8.88200954e-15,  2.98136937e-15, -1.50273163e-14, -5.83423871e-15,\n",
              "       -1.18429030e-14, -1.24704505e-14,  2.09268762e-14,  2.62833780e-14,\n",
              "        8.60005387e+01,  1.30027220e-14,  3.69784520e-14,  8.65735223e-15,\n",
              "       -2.94712878e-15, -1.34612353e-14,  1.51258486e-14,  4.70256538e-15,\n",
              "       -1.08426240e-14, -8.41513638e-15, -1.42350984e-14,  1.67063492e-14,\n",
              "       -1.71675166e-14, -3.75401556e-14,  2.03106661e-14, -3.94366026e-14,\n",
              "       -2.39657002e-14, -1.37124888e-14,  1.66218983e-14, -2.90324765e-15,\n",
              "        1.01198556e-14, -2.66859772e-14,  4.77862170e-14,  1.52809406e-14,\n",
              "        5.88313445e-15, -3.61047174e-14, -1.11887204e-14, -1.39786612e-14,\n",
              "       -2.94078241e-15, -6.21863056e-17,  2.77189749e-14, -2.11941998e-14,\n",
              "       -3.57332100e-15,  6.10701192e-16,  2.55651910e-04, -1.99017388e-15,\n",
              "        2.50070499e-14,  1.88656086e+01,  2.02039823e-14,  9.05665056e-15,\n",
              "        1.79920517e-15, -1.83288277e-15,  1.36218178e-14, -1.57103610e-14,\n",
              "        1.26353481e-14,  4.13081554e+01,  1.11099676e-14,  3.13281440e-14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLSauzvvL1WN"
      },
      "source": [
        "***Задание 1 (1 балл).*** Объясните, чем вызвана разница в значениях двух полученных значений метрики?\n",
        "\n",
        "***Задание 2 (1 балл).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kIZhnhqZEQI"
      },
      "source": [
        "Задание 1. Аналитическое решение изначально нацелено на поиск глобального минимума, в то время как метод градиентного спуска \n",
        "движется к глобальному минимумо по шажкам, что может привести к застреванию в локальных минимумах.\n",
        "Задание 2. Чтобы значение MSE было примерно равно при использовании обоих методов, необходимо уменьшить значение градиентного шага (параметр alpha).\n",
        "Чтобы получить примерно одинаковые значение MSE, alpha должно равняться 0.000000000000001 (но результат не всегда воспроизводим)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BayUnTUxL1WQ"
      },
      "source": [
        "## Ваша многомерная линейная регрессия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHTJ0xQfL1Wa"
      },
      "source": [
        "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс. \n",
        "\n",
        "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
        "\n",
        "***Задание 4 (2 балла)***. Добавьте l1 (первый и четвертый варианты) или l2 (второй и третий варианты) регуляризацию. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C_8IizdL1Wc"
      },
      "source": [
        "class LinearRegression(object):\n",
        "    def __init__(self, alpha=0.0001, l_ratio=0.001, tol=0.001, max_iter=1000):\n",
        "        '''\n",
        "        Для начала необходимо инициализировать параметры\n",
        "        alpha - это learning rate или шаг обучения\n",
        "        l_ratio - параметр регуляризации\n",
        "        tol - значение для критерия останова\n",
        "        max_iter - максимальное количество итераций обучения\n",
        "        '''\n",
        "        \n",
        "        self.alpha = alpha\n",
        "        self.l_ratio = l_ratio\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "             \n",
        "    def fit(self, X, y):\n",
        "        self.m, self.n = X.shape  \n",
        "        self.X = X \n",
        "        self.y = y\n",
        "        self.betta = np.zeros(self.n) \n",
        "        self.const = 0\n",
        "        self.iter = 0\n",
        "        while (np.abs(mean_squared_error(self.y, y_pred_now)-mean_squared_error(self.y, y_pred_pr))<self.tol) or (self.iter==self.max_iter) or (self.iter==0): \n",
        "            self.predict() \n",
        "        return X.dot(self.betta) + self.const\n",
        "   \n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Метод для предсказаний линейной регрессии\n",
        "        X - матрица признаков\n",
        "        '''\n",
        "        y_pred_pr = y_pred_now\n",
        "        y_pred_now = self.predict(self.X)  \n",
        "        gradient_betta = np.zeros(self.n)\n",
        "        for j in range(self.n): \n",
        "              gradient_betta[j] = (- (( self.X[:, j]).dot(self.y - y_pred_now))) / self.m \n",
        "        gradient_const = - np.sum(self.y - y_pred_now) / self.m  \n",
        "        # update weights \n",
        "        self.betta = self.betta - self.alpha * gradient_betta \n",
        "        self.const = self.const - self.alpha * gradient_const \n",
        "        return self "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzUL0j4kpunk"
      },
      "source": [
        "class LassoRegression(object) : \n",
        "    def __init__(self, alpha=0.0001, l_ratio=0.001, max_iter=1000): \n",
        "        self.alpha = alpha \n",
        "        self.l_ratio = l_ratio \n",
        "        self.max_iter = max_iter \n",
        "\n",
        "    def fit(self, X, Y ) : \n",
        "        self.m, self.n = X.shape \n",
        "        self.betta = np.zeros(self.n) \n",
        "        self.const = 0\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        for i in range(self.iterations): \n",
        "            self.predict() \n",
        "        return X.dot(self.betta) + self.const\n",
        "\n",
        "    def predict(self): \n",
        "        y_pred = self.predict( self.X )  \n",
        "        gradient_betta = np.zeros( self.n ) \n",
        "        for j in range(self.n): \n",
        "            if self.betta[j] > 0: \n",
        "                gradient_betta[j] = (- ( 2 * (self.X[:, j]).dot(self.y - y_pred)) + self.l_ratio) / self.m \n",
        "            else: \n",
        "                gradient_betta[j] = (- ( 2 * (self.X[:, j]).dot(self.y - y_pred)) - self.l_ratio) / self.m \n",
        "        gradient_const = - 2 * np.sum(self.y - y_pred) / self.m  \n",
        "        self.betta = self.betta - self.learning_rate * gradietn_betta \n",
        "        self.const = self.const - self.learning_rate * gradient_const \n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "151C4aTLL1Wm"
      },
      "source": [
        "my_reg = LinearRegression()\n",
        "my_reg.fit(X, y)\n",
        "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
        "print('You are amazing! Great work!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1wSfogIL1Wz"
      },
      "source": [
        "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки с l1-регуляризацией (from sklearn.linear_model import Lasso, первый и четвертый варианты) или с l2-регуляризацией (from sklearn.linear_model import Ridge, второй и третий варианты) с значением параметра регуляризации 0.1. Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFM97JJvL1W0"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}